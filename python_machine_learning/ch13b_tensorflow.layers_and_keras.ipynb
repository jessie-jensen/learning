{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import struct\n",
    "# from tensorflow.examples.tutorials.mnist import input_datat_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### mnist - multilayer perceptron w/ tensorflow layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_mnist(path, kind='train'):\n",
    "    kind = 't10k' if kind=='test' else kind\n",
    "    labels_path = os.path.join(path, '{}-labels-idx1-ubyte'.format(kind))\n",
    "    img_path = os.path.join(path, '{}-images-idx3-ubyte'.format(kind))\n",
    "    \n",
    "    with open(labels_path, 'rb') as label_p:\n",
    "        magic, n = struct.unpack('>II', label_p.read(8))\n",
    "        labels = np.fromfile(label_p, dtype=np.uint8)\n",
    "        \n",
    "    with open(img_path, 'rb') as img_p:\n",
    "        magic, n, rows, cols = struct.unpack('>IIII', img_p.read(16))\n",
    "        images = np.fromfile(img_p, dtype=np.uint8).reshape(len(labels), 784)\n",
    "        \n",
    "    return images, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, y_train = load_mnist('./mnist/', kind='train')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test, y_test = load_mnist('./mnist/', kind='t10k')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "## mean centering and normalization:\n",
    "mean_vals = np.mean(X_train, axis=0)\n",
    "std_val = np.std(X_train)\n",
    "\n",
    "X_train_centered = (X_train - mean_vals)/std_val\n",
    "X_test_centered = (X_test - mean_vals)/std_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_features = X_train_centered.shape[1]\n",
    "n_classes = 10\n",
    "random_seed = 123\n",
    "np.random.seed(random_seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "g = tf.Graph()\n",
    "\n",
    "with g.as_default():\n",
    "    tf.set_random_seed(random_seed)\n",
    "    \n",
    "    tf_x = tf.placeholder(dtype=tf.float32,\n",
    "                         shape=(None, n_features),\n",
    "                         name='tf_x')\n",
    "    tf_y = tf.placeholder(dtype=tf.int32,\n",
    "                         shape=None,\n",
    "                         name='tf_y')\n",
    "    y_onehot = tf.one_hot(indices=tf_y,\n",
    "                         depth=n_classes)\n",
    "    \n",
    "    h1 = tf.layers.dense(inputs=tf_x,\n",
    "                        units=50,\n",
    "                        activation=tf.tanh,\n",
    "                        name='layer1')\n",
    "    h2 = tf.layers.dense(inputs=h1,\n",
    "                        units=50,\n",
    "                        activation=tf.tanh,\n",
    "                        name='layer2')\n",
    "    logits = tf.layers.dense(inputs=h2,\n",
    "                            units=10,\n",
    "                            activation=None,\n",
    "                            name='layer3')\n",
    "    \n",
    "    predictions = {\n",
    "        'classes': tf.argmax(logits,\n",
    "                            axis=1,\n",
    "                            name='pred_class'),\n",
    "        'probabilities': tf.nn.softmax(logits,\n",
    "                                      name='softmax_tensor')\n",
    "    } "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /Users/jj/anaconda3/lib/python3.6/site-packages/tensorflow/python/ops/losses/losses_impl.py:209: to_float (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n"
     ]
    }
   ],
   "source": [
    "with g.as_default():\n",
    "    cost = tf.losses.softmax_cross_entropy(onehot_labels=y_onehot, logits=logits)\n",
    "    \n",
    "    optimizer = tf.train.GradientDescentOptimizer(learning_rate=0.001)\n",
    "    \n",
    "    train_op = optimizer.minimize(loss=cost)\n",
    "    \n",
    "    init_op = tf.global_variables_initializer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_batch_generator(X, y, batch_size=128, shuffle=False):\n",
    "    X_copy = np.array(X)\n",
    "    y_copy = np.array(y)\n",
    "    \n",
    "    if shuffle:\n",
    "        data = np.column_stack((X_copy, y_copy))\n",
    "        np.random.shuffle(data)\n",
    "        X_copy = data[:, :-1]\n",
    "        y_copy = data[:, -1].astype(int)\n",
    "        \n",
    "    for i in range(0, X.shape[0], batch_size):\n",
    "        yield (X_copy[i:i+batch_size, :], y_copy[i:i+batch_size])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-- EPOCH 1 -- Avg. Cost: 1.5572493076324463\n",
      "-- EPOCH 2 -- Avg. Cost: 0.9490127563476562\n",
      "-- EPOCH 3 -- Avg. Cost: 0.7497641444206238\n",
      "-- EPOCH 4 -- Avg. Cost: 0.6385995149612427\n",
      "-- EPOCH 5 -- Avg. Cost: 0.5667569637298584\n",
      "-- EPOCH 6 -- Avg. Cost: 0.5158420205116272\n",
      "-- EPOCH 7 -- Avg. Cost: 0.4780220091342926\n",
      "-- EPOCH 8 -- Avg. Cost: 0.4484705626964569\n",
      "-- EPOCH 9 -- Avg. Cost: 0.4247126281261444\n",
      "-- EPOCH 10 -- Avg. Cost: 0.405110239982605\n",
      "-- EPOCH 11 -- Avg. Cost: 0.3884292244911194\n",
      "-- EPOCH 12 -- Avg. Cost: 0.3742930293083191\n",
      "-- EPOCH 13 -- Avg. Cost: 0.36180493235588074\n",
      "-- EPOCH 14 -- Avg. Cost: 0.3507132828235626\n",
      "-- EPOCH 15 -- Avg. Cost: 0.3408225476741791\n",
      "-- EPOCH 16 -- Avg. Cost: 0.331996887922287\n",
      "-- EPOCH 17 -- Avg. Cost: 0.32398995757102966\n",
      "-- EPOCH 18 -- Avg. Cost: 0.31648558378219604\n",
      "-- EPOCH 19 -- Avg. Cost: 0.30970874428749084\n",
      "-- EPOCH 20 -- Avg. Cost: 0.30342885851860046\n",
      "-- EPOCH 21 -- Avg. Cost: 0.29759082198143005\n",
      "-- EPOCH 22 -- Avg. Cost: 0.29222217202186584\n",
      "-- EPOCH 23 -- Avg. Cost: 0.2870437800884247\n",
      "-- EPOCH 24 -- Avg. Cost: 0.28225579857826233\n",
      "-- EPOCH 25 -- Avg. Cost: 0.27769604325294495\n",
      "-- EPOCH 26 -- Avg. Cost: 0.2734173834323883\n",
      "-- EPOCH 27 -- Avg. Cost: 0.26924657821655273\n",
      "-- EPOCH 28 -- Avg. Cost: 0.26533836126327515\n",
      "-- EPOCH 29 -- Avg. Cost: 0.26168879866600037\n",
      "-- EPOCH 30 -- Avg. Cost: 0.2580806612968445\n",
      "-- EPOCH 31 -- Avg. Cost: 0.254843533039093\n",
      "-- EPOCH 32 -- Avg. Cost: 0.2515631318092346\n",
      "-- EPOCH 33 -- Avg. Cost: 0.24849672615528107\n",
      "-- EPOCH 34 -- Avg. Cost: 0.24530842900276184\n",
      "-- EPOCH 35 -- Avg. Cost: 0.24245679378509521\n",
      "-- EPOCH 36 -- Avg. Cost: 0.23979151248931885\n",
      "-- EPOCH 37 -- Avg. Cost: 0.23707659542560577\n",
      "-- EPOCH 38 -- Avg. Cost: 0.23451761901378632\n",
      "-- EPOCH 39 -- Avg. Cost: 0.2318522185087204\n",
      "-- EPOCH 40 -- Avg. Cost: 0.2294195592403412\n",
      "-- EPOCH 41 -- Avg. Cost: 0.22708897292613983\n",
      "-- EPOCH 42 -- Avg. Cost: 0.22482572495937347\n",
      "-- EPOCH 43 -- Avg. Cost: 0.22259031236171722\n",
      "-- EPOCH 44 -- Avg. Cost: 0.22052797675132751\n",
      "-- EPOCH 45 -- Avg. Cost: 0.2184038609266281\n",
      "-- EPOCH 46 -- Avg. Cost: 0.21626350283622742\n",
      "-- EPOCH 47 -- Avg. Cost: 0.2142753005027771\n",
      "-- EPOCH 48 -- Avg. Cost: 0.21247252821922302\n",
      "-- EPOCH 49 -- Avg. Cost: 0.21055245399475098\n",
      "-- EPOCH 50 -- Avg. Cost: 0.2088434398174286\n"
     ]
    }
   ],
   "source": [
    "epochs = 50\n",
    "\n",
    "sess = tf.Session(graph=g)\n",
    "sess.run(init_op)\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    training_costs = []\n",
    "    \n",
    "    batch_gen = create_batch_generator(X_train_centered, y_train, batch_size=64, shuffle=True)\n",
    "    \n",
    "    for X_batch, y_batch in batch_gen:\n",
    "        feed = {tf_x:X_batch, tf_y:y_batch}\n",
    "        _, batch_cost = sess.run([train_op, cost], feed_dict=feed)\n",
    "        training_costs.append(batch_cost)\n",
    "        \n",
    "    print('-- EPOCH {} -- Avg. Cost: {}'.format(epoch+1, np.mean(training_costs)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test accuracy:  0.9388\n"
     ]
    }
   ],
   "source": [
    "y_pred = sess.run(predictions['classes'], feed_dict={tf_x:X_test_centered})\n",
    "\n",
    "print('test accuracy: ',\n",
    "     np.sum(y_pred == y_test)/y_test.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### mnist - multilayer perceptron w/ keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow.contrib.keras as keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(random_seed)\n",
    "tf.set_random_seed(random_seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_onehot = keras.utils.to_categorical(y_train)\n",
    "n_classes = y_train_onehot.shape[1]\n",
    "\n",
    "model = keras.models.Sequential()\n",
    "\n",
    "model.add(keras.layers.Dense(units=50,\n",
    "                            input_dim=n_features,\n",
    "                            kernel_initializer='glorot_uniform',\n",
    "                            bias_initializer='zeros',\n",
    "                            activation='tanh'))\n",
    "model.add(keras.layers.Dense(units=50,\n",
    "                            input_dim=50,\n",
    "                            kernel_initializer='glorot_uniform',\n",
    "                            bias_initializer='zeros',\n",
    "                            activation='tanh'))\n",
    "model.add(keras.layers.Dense(units=n_classes,\n",
    "                            input_dim=50,\n",
    "                            kernel_initializer='glorot_uniform',\n",
    "                            bias_initializer='zeros',\n",
    "                            activation='softmax'))\n",
    "\n",
    "sgd_optimizer = keras.optimizers.SGD(lr=0.001,\n",
    "                                    decay=1e-7,\n",
    "                                    momentum=.9)\n",
    "\n",
    "# compile\n",
    "model.compile(optimizer=sgd_optimizer,\n",
    "             loss='categorical_crossentropy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 54000 samples, validate on 6000 samples\n",
      "WARNING:tensorflow:From /Users/jj/anaconda3/lib/python3.6/site-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "Epoch 1/50\n",
      "54000/54000 [==============================] - 1s 26us/sample - loss: 0.7422 - val_loss: 0.3736\n",
      "Epoch 2/50\n",
      "54000/54000 [==============================] - 1s 22us/sample - loss: 0.3781 - val_loss: 0.2805\n",
      "Epoch 3/50\n",
      "54000/54000 [==============================] - 1s 22us/sample - loss: 0.3109 - val_loss: 0.2411\n",
      "Epoch 4/50\n",
      "54000/54000 [==============================] - 1s 23us/sample - loss: 0.2737 - val_loss: 0.2171\n",
      "Epoch 5/50\n",
      "54000/54000 [==============================] - 1s 22us/sample - loss: 0.2478 - val_loss: 0.1999\n",
      "Epoch 6/50\n",
      "54000/54000 [==============================] - 1s 23us/sample - loss: 0.2276 - val_loss: 0.1842\n",
      "Epoch 7/50\n",
      "54000/54000 [==============================] - 1s 22us/sample - loss: 0.2110 - val_loss: 0.1732\n",
      "Epoch 8/50\n",
      "54000/54000 [==============================] - 1s 23us/sample - loss: 0.1973 - val_loss: 0.1640\n",
      "Epoch 9/50\n",
      "54000/54000 [==============================] - 1s 23us/sample - loss: 0.1854 - val_loss: 0.1562\n",
      "Epoch 10/50\n",
      "54000/54000 [==============================] - 1s 23us/sample - loss: 0.1753 - val_loss: 0.1491\n",
      "Epoch 11/50\n",
      "54000/54000 [==============================] - 1s 23us/sample - loss: 0.1661 - val_loss: 0.1429\n",
      "Epoch 12/50\n",
      "54000/54000 [==============================] - 1s 23us/sample - loss: 0.1580 - val_loss: 0.1387\n",
      "Epoch 13/50\n",
      "54000/54000 [==============================] - 1s 23us/sample - loss: 0.1507 - val_loss: 0.1336\n",
      "Epoch 14/50\n",
      "54000/54000 [==============================] - 1s 23us/sample - loss: 0.1441 - val_loss: 0.1301\n",
      "Epoch 15/50\n",
      "54000/54000 [==============================] - 1s 23us/sample - loss: 0.1380 - val_loss: 0.1275\n",
      "Epoch 16/50\n",
      "54000/54000 [==============================] - 1s 23us/sample - loss: 0.1325 - val_loss: 0.1239\n",
      "Epoch 17/50\n",
      "54000/54000 [==============================] - 1s 23us/sample - loss: 0.1274 - val_loss: 0.1212\n",
      "Epoch 18/50\n",
      "54000/54000 [==============================] - 1s 26us/sample - loss: 0.1226 - val_loss: 0.1189\n",
      "Epoch 19/50\n",
      "54000/54000 [==============================] - 1s 23us/sample - loss: 0.1181 - val_loss: 0.1174\n",
      "Epoch 20/50\n",
      "54000/54000 [==============================] - 1s 23us/sample - loss: 0.1140 - val_loss: 0.1153\n",
      "Epoch 21/50\n",
      "54000/54000 [==============================] - 1s 25us/sample - loss: 0.1101 - val_loss: 0.1133\n",
      "Epoch 22/50\n",
      "54000/54000 [==============================] - 1s 24us/sample - loss: 0.1064 - val_loss: 0.1131\n",
      "Epoch 23/50\n",
      "54000/54000 [==============================] - 1s 23us/sample - loss: 0.1030 - val_loss: 0.1110\n",
      "Epoch 24/50\n",
      "54000/54000 [==============================] - 1s 22us/sample - loss: 0.0997 - val_loss: 0.1101\n",
      "Epoch 25/50\n",
      "54000/54000 [==============================] - 1s 22us/sample - loss: 0.0966 - val_loss: 0.1092\n",
      "Epoch 26/50\n",
      "54000/54000 [==============================] - 1s 23us/sample - loss: 0.0937 - val_loss: 0.1085\n",
      "Epoch 27/50\n",
      "54000/54000 [==============================] - 1s 23us/sample - loss: 0.0909 - val_loss: 0.1077\n",
      "Epoch 28/50\n",
      "54000/54000 [==============================] - 1s 22us/sample - loss: 0.0881 - val_loss: 0.1063\n",
      "Epoch 29/50\n",
      "54000/54000 [==============================] - 1s 22us/sample - loss: 0.0857 - val_loss: 0.1052\n",
      "Epoch 30/50\n",
      "54000/54000 [==============================] - 1s 23us/sample - loss: 0.0833 - val_loss: 0.1050\n",
      "Epoch 31/50\n",
      "54000/54000 [==============================] - 1s 22us/sample - loss: 0.0809 - val_loss: 0.1052\n",
      "Epoch 32/50\n",
      "54000/54000 [==============================] - 1s 22us/sample - loss: 0.0787 - val_loss: 0.1042\n",
      "Epoch 33/50\n",
      "54000/54000 [==============================] - 1s 22us/sample - loss: 0.0765 - val_loss: 0.1043\n",
      "Epoch 34/50\n",
      "54000/54000 [==============================] - 1s 22us/sample - loss: 0.0745 - val_loss: 0.1038\n",
      "Epoch 35/50\n",
      "54000/54000 [==============================] - 1s 22us/sample - loss: 0.0724 - val_loss: 0.1035\n",
      "Epoch 36/50\n",
      "54000/54000 [==============================] - 1s 22us/sample - loss: 0.0706 - val_loss: 0.1037\n",
      "Epoch 37/50\n",
      "54000/54000 [==============================] - 1s 22us/sample - loss: 0.0686 - val_loss: 0.1038\n",
      "Epoch 38/50\n",
      "54000/54000 [==============================] - 1s 22us/sample - loss: 0.0668 - val_loss: 0.1029\n",
      "Epoch 39/50\n",
      "54000/54000 [==============================] - 1s 22us/sample - loss: 0.0652 - val_loss: 0.1022\n",
      "Epoch 40/50\n",
      "54000/54000 [==============================] - 1s 22us/sample - loss: 0.0634 - val_loss: 0.1023\n",
      "Epoch 41/50\n",
      "54000/54000 [==============================] - 1s 22us/sample - loss: 0.0618 - val_loss: 0.1024\n",
      "Epoch 42/50\n",
      "54000/54000 [==============================] - 1s 22us/sample - loss: 0.0601 - val_loss: 0.1028\n",
      "Epoch 43/50\n",
      "54000/54000 [==============================] - 1s 22us/sample - loss: 0.0587 - val_loss: 0.1035\n",
      "Epoch 44/50\n",
      "54000/54000 [==============================] - 1s 22us/sample - loss: 0.0573 - val_loss: 0.1029\n",
      "Epoch 45/50\n",
      "54000/54000 [==============================] - 1s 22us/sample - loss: 0.0557 - val_loss: 0.1019\n",
      "Epoch 46/50\n",
      "54000/54000 [==============================] - 1s 22us/sample - loss: 0.0544 - val_loss: 0.1030\n",
      "Epoch 47/50\n",
      "54000/54000 [==============================] - 1s 22us/sample - loss: 0.0530 - val_loss: 0.1032\n",
      "Epoch 48/50\n",
      "54000/54000 [==============================] - 1s 23us/sample - loss: 0.0517 - val_loss: 0.1032\n",
      "Epoch 49/50\n",
      "54000/54000 [==============================] - 1s 23us/sample - loss: 0.0504 - val_loss: 0.1040\n",
      "Epoch 50/50\n",
      "54000/54000 [==============================] - 1s 23us/sample - loss: 0.0492 - val_loss: 0.1030\n"
     ]
    }
   ],
   "source": [
    "train_log = model.fit(X_train_centered, y_train_onehot,\n",
    "                     batch_size=64,\n",
    "                     epochs=50,\n",
    "                     verbose=True,\n",
    "                     validation_split=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "60000/60000 [==============================] - 1s 10us/sample\n",
      "training accuracy:  0.9881333333333333\n"
     ]
    }
   ],
   "source": [
    "y_train_pred = model.predict_classes(X_train_centered, verbose=True)\n",
    "print('training accuracy: ', np.sum(y_train == y_train_pred, axis=0) / y_train.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000/10000 [==============================] - 0s 12us/sample\n",
      "test accuracy:  0.9627\n"
     ]
    }
   ],
   "source": [
    "y_test_pred = model.predict_classes(X_test_centered, verbose=True)\n",
    "print('test accuracy: ', np.sum(y_test == y_test_pred, axis=0) / y_test.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
